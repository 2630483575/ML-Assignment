"""
Script to generate all visualization plots for the NER project.

This script creates comprehensive visualizations including:
- Model performance comparisons
- Confusion matrices
- Training history plots
- Error analysis
- Summary report
"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
import json
import numpy as np
from utils.enhanced_visualizations import (
    plot_model_comparison,
    plot_confusion_matrices,
    plot_training_curves,
    plot_error_analysis,
    plot_per_label_performance,
    create_summary_report
)


def load_model_results(output_dir='outputs'):
    """
    Load results from all trained models.
    
    Returns:
        Dictionary mapping model names to results
    """
    results = {}
    
    # Try to load results from various sources
    model_files = {
        'CRF': 'crf_results.json',
        'BiLSTM': 'bilstm_results.json',
        'BERT': 'bert_results.json',
        'RoBERTa': 'roberta_results.json'
    }
    
    for model_name, filename in model_files.items():
        filepath = os.path.join(output_dir, filename)
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                results[model_name] = json.load(f)
        else:
            print(f"Warning: {filename} not found, skipping {model_name}")
    
    # If no results found, use example data for demonstration
    if not results:
        print("No model results found, using example data...")
        results = {
            'CRF': {'f1': 0.8117, 'precision': 0.8196, 'recall': 0.8040},
            'BiLSTM': {'f1': 0.8842, 'precision': 0.8891, 'recall': 0.8794},
            'BERT': {'f1': 0.9123, 'precision': 0.9201, 'recall': 0.9046},
            'RoBERTa': {'f1': 0.9201, 'precision': 0.9264, 'recall': 0.9139}
        }
    
    return results


def generate_all_visualizations(output_dir='outputs', vis_dir=None):
    """
    Generate all visualization plots.
    
    Args:
        output_dir: Directory containing model results
        vis_dir: Directory to save visualizations (default: output_dir/visualizations)
    """
    if vis_dir is None:
        vis_dir = os.path.join(output_dir, 'visualizations')
    
    os.makedirs(vis_dir, exist_ok=True)
    
    print("="*80)
    print("GENERATING COMPREHENSIVE VISUALIZATION SUITE")
    print("="*80)
    
    # Load results
    print("\n1. Loading model results...")
    results = load_model_results(output_dir)
    print(f"   Loaded results for {len(results)} models: {', '.join(results.keys())}")
    
    # 1. Model Comparison
    print("\n2. Creating model comparison plot...")
    plot_model_comparison(
        results,
        metrics=['f1', 'precision', 'recall'],
        save_path=os.path.join(vis_dir, 'model_comparison.png')
    )
    
    # 2. Confusion Matrices (if predictions available)
    print("\n3. Checking for confusion matrix data...")
    prediction_files = {
        'CRF': os.path.join(output_dir, 'crf_predictions.json'),
        'BiLSTM': os.path.join(output_dir, 'bilstm_predictions.json'),
        'BERT': os.path.join(output_dir, 'bert_predictions.json'),
        'RoBERTa': os.path.join(output_dir, 'roberta_predictions.json')
    }

    y_true_dict = {}
    y_pred_dict = {}
    all_labels = set()

    for model_name, filepath in prediction_files.items():
        if os.path.exists(filepath):
            print(f"   Loading predictions for {model_name}...")
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    y_true_dict[model_name] = data['y_true']
                    y_pred_dict[model_name] = data['y_pred']
                    
                    # Collect all unique labels
                    for sent in data['y_true']:
                        all_labels.update(sent)
            except Exception as e:
                print(f"   Error loading {filepath}: {e}")

    if y_true_dict:
        print(f"   Generating confusion matrices for {len(y_true_dict)} models...")
        # Sort labels for consistent ordering, ensuring 'O' is last or handled appropriately
        label_list = sorted(list(all_labels))
        if 'O' in label_list:
            label_list.remove('O')
            label_list.append('O')
            
        plot_confusion_matrices(
            y_true_dict,
            y_pred_dict,
            label_list=label_list,
            save_path=os.path.join(vis_dir, 'confusion_matrices.png')
        )
    else:
        print("   Skipped (no prediction data found)")
    
    # 3. Training Curves (if history available)
    print("\n4. Checking for training history...")
    history_files = {
        'BiLSTM': os.path.join(output_dir, 'bilstm_history.json'),
        'BERT': os.path.join(output_dir, 'bert_history.json'),
        'RoBERTa': os.path.join(output_dir, 'roberta_history.json')
    }
    
    history_dict = {}
    for model_name, filepath in history_files.items():
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                history_dict[model_name] = json.load(f)
    
    if history_dict:
        print(f"   Creating training curves for {len(history_dict)} models...")
        plot_training_curves(
            history_dict,
            save_path=os.path.join(vis_dir, 'training_curves.png')
        )
    else:
        print("   Skipped (no history data found)")
    
    # 4. Per-label Performance (example)
    print("\n5. Creating per-label performance plot...")
    # Example per-label metrics
    metrics_per_label = {
        'PER': {'precision': 0.95, 'recall': 0.94, 'f1': 0.945},
        'ORG': {'precision': 0.88, 'recall': 0.86, 'f1': 0.870},
        'LOC': {'precision': 0.92, 'recall': 0.91, 'f1': 0.915},
        'MISC': {'precision': 0.81, 'recall': 0.79, 'f1': 0.800}
    }
    plot_per_label_performance(
        metrics_per_label,
        save_path=os.path.join(vis_dir, 'per_label_performance.png')
    )
    
    # 5. Summary Report
    print("\n6. Creating comprehensive summary report...")
    create_summary_report(
        results,
        save_path=os.path.join(vis_dir, 'summary_report.png')
    )
    
    print("\n" + "="*80)
    print("VISUALIZATION GENERATION COMPLETE!")
    print(f"All plots saved to: {vis_dir}")
    print("="*80)
    
    # Create index HTML for easy viewing
    create_visualization_index(vis_dir)


def create_visualization_index(vis_dir):
    """Create an HTML index page for all visualizations."""
    html_content = """
<!DOCTYPE html>
<html>
<head>
    <title>NER Project Visualizations</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        h1 {{
            color: #333;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #555;
            margin-top: 30px;
        }}
        .visualization {{
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        img {{
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }}
        .description {{
            color: #666;
            font-size: 14px;
            margin-top: 10px;
        }}
    </style>
</head>
<body>
    <h1>ðŸŽ¨ NER Project Visualization Gallery</h1>
    <p>Comprehensive visualizations for the Named Entity Recognition project.</p>
    
    <div class="visualization">
        <h2>1. Model Performance Comparison</h2>
        <img src="model_comparison.png" alt="Model Comparison">
        <p class="description">
            Side-by-side comparison of all models showing F1, Precision, and Recall scores.
            The right panel shows models ranked by F1 score with the best model highlighted in gold.
        </p>
    </div>
    
    <div class="visualization">
        <h2>2. Training Curves</h2>
        <img src="training_curves.png" alt="Training Curves">
        <p class="description">
            Training and validation metrics across epochs for BiLSTM, BERT, and RoBERTa.
            Includes loss curves and learning rate schedules.
        </p>
    </div>
    
    <div class="visualization">
        <h2>3. Per-Label Performance</h2>
        <img src="per_label_performance.png" alt="Per-Label Performance">
        <p class="description">
            Performance breakdown by entity type (PER, ORG, LOC, MISC).
            Shows which entity types are easier or harder to recognize.
        </p>
    </div>
    
    <div class="visualization">
        <h2>4. Summary Report</h2>
        <img src="summary_report.png" alt="Summary Report">
        <p class="description">
            Comprehensive summary dashboard with multiple views:
            F1 comparison, Precision-Recall scatter, best model radar chart, and detailed metrics table.
        </p>
    </div>
    
    <hr style="margin: 40px 0;">
    <p style="text-align: center; color: #999; font-size: 12px;">
        Generated automatically by generate_visualizations.py
    </p>
</body>
</html>
"""
    
    index_path = os.path.join(vis_dir, 'index.html')
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"\nðŸ“„ Visualization index created: {index_path}")
    print(f"   Open this file in a browser to view all visualizations!")


def main():
    parser = argparse.ArgumentParser(description="Generate NER project visualizations")
    parser.add_argument("--output_dir", type=str, default="outputs",
                       help="Directory containing model results")
    parser.add_argument("--vis_dir", type=str, default=None,
                       help="Directory to save visualizations")
    
    args = parser.parse_args()
    
    generate_all_visualizations(args.output_dir, args.vis_dir)


if __name__ == "__main__":
    main()
